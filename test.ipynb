{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from models.GigaCount import GigaCount\n",
    "from dataset import Crowd\n",
    "from utils_custom import collate_fn\n",
    "from transforms import RandomResizedCrop\n",
    "from torchvision.transforms.v2 import Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torchvision.transforms.functional import normalize, to_pil_image\n",
    "from utils_custom import resize_density_map\n",
    "from eval_metrics import sliding_window_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = Crowd(\"qnrf\", split=\"val\", transforms=None, sigma=None, return_filename=True)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "data_iter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BINS SHAPE:  [(0.0, 0.0), (1.0, 1.0), (2.0, 2.0), (3.0, 3.0), (4.0, inf)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GigaCount(\n",
       "  (image_encoder): Backbone(\n",
       "    (stem): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (stage1): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (stage2): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "        )\n",
       "        (3): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "        )\n",
       "        (4): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "        )\n",
       "        (5): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "        )\n",
       "        (6): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "        )\n",
       "        (7): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "        )\n",
       "        (8): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ccsm1): ccsm(\n",
       "    (ch_att_s): ChannelAttention(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (shared_MLP): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (sa_s): SpatialAttention(\n",
       "      (conv1): Conv2d(1, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (conv1): Sequential(\n",
       "      (0): ODConv2d(\n",
       "        (attention): Attention(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (channel_fc): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (filter_fc): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (kernel_fc): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): ODConv2d(\n",
       "        (attention): Attention(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (channel_fc): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (filter_fc): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (kernel_fc): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): ODConv2d(\n",
       "        (attention): Attention(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (channel_fc): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (filter_fc): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (kernel_fc): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): ODConv2d(\n",
       "        (attention): Attention(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (channel_fc): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (filter_fc): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (kernel_fc): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (ccsm2): ccsm(\n",
       "    (ch_att_s): ChannelAttention(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (shared_MLP): Sequential(\n",
       "        (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (sa_s): SpatialAttention(\n",
       "      (conv1): Conv2d(1, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (conv1): Sequential(\n",
       "      (0): ODConv2d(\n",
       "        (attention): Attention(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (channel_fc): Conv2d(16, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (filter_fc): Conv2d(16, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (kernel_fc): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): ODConv2d(\n",
       "        (attention): Attention(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (channel_fc): Conv2d(16, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (filter_fc): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (kernel_fc): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): ODConv2d(\n",
       "        (attention): Attention(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (channel_fc): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (filter_fc): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (kernel_fc): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): ODConv2d(\n",
       "        (attention): Attention(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (channel_fc): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (filter_fc): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (kernel_fc): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (ccsm3): ccsm(\n",
       "    (ch_att_s): ChannelAttention(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (shared_MLP): Sequential(\n",
       "        (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (sa_s): SpatialAttention(\n",
       "      (conv1): Conv2d(1, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (conv1): Sequential(\n",
       "      (0): ODConv2d(\n",
       "        (attention): Attention(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (channel_fc): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (filter_fc): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (kernel_fc): Conv2d(24, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): ODConv2d(\n",
       "        (attention): Attention(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (channel_fc): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (filter_fc): Conv2d(24, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (kernel_fc): Conv2d(24, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): ODConv2d(\n",
       "        (attention): Attention(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (channel_fc): Conv2d(16, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (filter_fc): Conv2d(16, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (kernel_fc): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): ODConv2d(\n",
       "        (attention): Attention(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (channel_fc): Conv2d(16, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (filter_fc): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (kernel_fc): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (fusion): FeatureFusion(\n",
       "    (upsample_1): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (upsample_2): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (final): Sequential(\n",
       "      (0): Conv2d(112, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (image_decoder): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(112, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(112, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projection): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (text_encoder): CLIPTextEncoder(\n",
       "    (token_embedding): Embedding(49408, 512)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"checkpoints/best_model.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "original_bins = {\n",
    "        \"qnrf\": {\n",
    "            \"bins\": {\n",
    "                \"fine\": [[0, 0], [1, 1], [2, 2], [3, 3], [4, \"inf\"]]\n",
    "            },\n",
    "            \"anchor_points\": {\n",
    "                \"fine\": {\n",
    "                    \"middle\": [0, 1, 2, 3, 4],\n",
    "                    \"average\": [0, 1, 2, 3, 4.21937]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "bins = original_bins[\"qnrf\"][\"bins\"][\"fine\"]\n",
    "anchor_points = original_bins[\"qnrf\"][\"anchor_points\"][\"fine\"][\"average\"] \n",
    "bins = [(float(b[0]), float(b[1]) if b[1] != \"inf\" else float('inf')) for b in bins]\n",
    "print(\"BINS SHAPE: \", bins)\n",
    "anchor_points = [float(p) for p in anchor_points]\n",
    "#bins = compute_dynamic_bins(original_bins[\"qnrf\"][\"bins\"], num_bins=3)\n",
    "#anchor_points = original_bins[\"qnrf\"][\"anchor_points\"][\"middle\"]\n",
    "\n",
    "model = GigaCount(bins=bins, anchor_points=anchor_points).to(DEVICE)\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)\n",
    "model.load_state_dict(checkpoint) \n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -304.03729248046875 527.873779296875\n",
      "After stage2 NaN: False\n",
      "torch.Size([20, 96, 56, 56]) torch.Size([20, 192, 28, 28]) torch.Size([20, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(93.2078, device='cuda:0')\n",
      "Min of x before attention: tensor(-36.0024, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(96.9354, device='cuda:0')\n",
      "Min of x before attention: tensor(-37.1678, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(28.7619, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(28.3144, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(263.9369, device='cuda:0')\n",
      "Min of x before attention: tensor(-197.8556, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(280.5094, device='cuda:0')\n",
      "Min of x before attention: tensor(-190.5725, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(30.6105, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(48.4013, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1811.3757, device='cuda:0')\n",
      "Min of x before attention: tensor(-2431.9780, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1811.3916, device='cuda:0')\n",
      "Min of x before attention: tensor(-2432.9006, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(5.8199, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0515, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(12.5771, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2182, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.19714264571666718 max: 0.2159448266029358 std: 0.04418696463108063\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 9.153308333509358e-09 max: 0.9999971389770508\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -320.2979431152344 577.04443359375\n",
      "After stage2 NaN: False\n",
      "torch.Size([15, 96, 56, 56]) torch.Size([15, 192, 28, 28]) torch.Size([15, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(80.6395, device='cuda:0')\n",
      "Min of x before attention: tensor(-33.3407, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(80.7001, device='cuda:0')\n",
      "Min of x before attention: tensor(-34.5060, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(30.8440, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(33.3059, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(288.5222, device='cuda:0')\n",
      "Min of x before attention: tensor(-283.8892, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(302.7563, device='cuda:0')\n",
      "Min of x before attention: tensor(-283.2030, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(24.7574, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(34.2129, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1821.8596, device='cuda:0')\n",
      "Min of x before attention: tensor(-2452.5933, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1821.8755, device='cuda:0')\n",
      "Min of x before attention: tensor(-2453.4475, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(4.7325, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.7404, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(7.2388, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2182, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.20096151530742645 max: 0.21822278201580048 std: 0.04418744146823883\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 8.732037315439811e-09 max: 0.9999954700469971\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -277.95208740234375 421.410400390625\n",
      "After stage2 NaN: False\n",
      "torch.Size([16, 96, 56, 56]) torch.Size([16, 192, 28, 28]) torch.Size([16, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(91.1456, device='cuda:0')\n",
      "Min of x before attention: tensor(-30.0446, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(93.3726, device='cuda:0')\n",
      "Min of x before attention: tensor(-31.2099, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(25.1093, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(33.0630, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(210.7052, device='cuda:0')\n",
      "Min of x before attention: tensor(-274.8322, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(212.3544, device='cuda:0')\n",
      "Min of x before attention: tensor(-274.3899, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(25.0768, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(32.1661, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1804.5105, device='cuda:0')\n",
      "Min of x before attention: tensor(-2434.0066, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1804.5264, device='cuda:0')\n",
      "Min of x before attention: tensor(-2435.0349, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(5.6727, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.9989, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(11.4703, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2182, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.21091119945049286 max: 0.21764160692691803 std: 0.04418420419096947\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 1.4238044521164284e-08 max: 0.9999901056289673\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -284.1050109863281 573.9781494140625\n",
      "After stage2 NaN: False\n",
      "torch.Size([70, 96, 56, 56]) torch.Size([70, 192, 28, 28]) torch.Size([70, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(71.0643, device='cuda:0')\n",
      "Min of x before attention: tensor(-35.6758, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(71.3976, device='cuda:0')\n",
      "Min of x before attention: tensor(-36.8411, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(23.7614, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(23.9402, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(286.9891, device='cuda:0')\n",
      "Min of x before attention: tensor(-198.9325, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(312.9585, device='cuda:0')\n",
      "Min of x before attention: tensor(-195.5278, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(34.6835, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(55.0155, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1855.6560, device='cuda:0')\n",
      "Min of x before attention: tensor(-2491.7588, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1855.6719, device='cuda:0')\n",
      "Min of x before attention: tensor(-2492.7483, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5103, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2731, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(30.5041, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.3271, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.18974174559116364 max: 0.2102925330400467 std: 0.04419172182679176\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 2.6565856181548497e-09 max: 0.9999997615814209\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -338.3316650390625 597.248291015625\n",
      "After stage2 NaN: False\n",
      "torch.Size([70, 96, 56, 56]) torch.Size([70, 192, 28, 28]) torch.Size([70, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(94.3057, device='cuda:0')\n",
      "Min of x before attention: tensor(-34.1046, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(102.0892, device='cuda:0')\n",
      "Min of x before attention: tensor(-35.2699, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(39.1965, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(43.5589, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(551.6516, device='cuda:0')\n",
      "Min of x before attention: tensor(-283.7547, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(563.9602, device='cuda:0')\n",
      "Min of x before attention: tensor(-280.2257, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(38.5691, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(90.3334, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1776.4100, device='cuda:0')\n",
      "Min of x before attention: tensor(-2367.3079, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1776.4259, device='cuda:0')\n",
      "Min of x before attention: tensor(-2368.5271, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5337, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2673, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(59.0760, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.5128, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.19977137446403503 max: 0.21815676987171173 std: 0.04419071599841118\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 2.2148534117860663e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -316.29266357421875 524.7457885742188\n",
      "After stage2 NaN: False\n",
      "torch.Size([70, 96, 56, 56]) torch.Size([70, 192, 28, 28]) torch.Size([70, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(95.7970, device='cuda:0')\n",
      "Min of x before attention: tensor(-29.6226, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(95.4416, device='cuda:0')\n",
      "Min of x before attention: tensor(-30.7880, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(22.4305, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(33.5593, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(311.9207, device='cuda:0')\n",
      "Min of x before attention: tensor(-236.4671, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(318.3477, device='cuda:0')\n",
      "Min of x before attention: tensor(-231.5069, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(36.0632, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(70.8685, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1786.5732, device='cuda:0')\n",
      "Min of x before attention: tensor(-2375.8657, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1786.5891, device='cuda:0')\n",
      "Min of x before attention: tensor(-2377.0588, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5475, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2986, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(78.4096, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.5574, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.19840678572654724 max: 0.2124580293893814 std: 0.04419204965233803\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 3.4038980523121154e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -381.21612548828125 699.214599609375\n",
      "After stage2 NaN: False\n",
      "torch.Size([15, 96, 56, 56]) torch.Size([15, 192, 28, 28]) torch.Size([15, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(88.4430, device='cuda:0')\n",
      "Min of x before attention: tensor(-42.1890, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(89.9999, device='cuda:0')\n",
      "Min of x before attention: tensor(-43.0640, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(20.9748, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(24.6630, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(488.7177, device='cuda:0')\n",
      "Min of x before attention: tensor(-247.0534, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(491.4347, device='cuda:0')\n",
      "Min of x before attention: tensor(-233.7665, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(23.7735, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(41.1562, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1807.7316, device='cuda:0')\n",
      "Min of x before attention: tensor(-2410.9585, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1807.7474, device='cuda:0')\n",
      "Min of x before attention: tensor(-2411.9460, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.1186, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1621, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(17.3636, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2394, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.19868147373199463 max: 0.22020451724529266 std: 0.04419287294149399\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 6.9746959319161306e-09 max: 0.999998927116394\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -261.9756164550781 567.3054809570312\n",
      "After stage2 NaN: False\n",
      "torch.Size([70, 96, 56, 56]) torch.Size([70, 192, 28, 28]) torch.Size([70, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(73.1550, device='cuda:0')\n",
      "Min of x before attention: tensor(-29.3059, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(75.0107, device='cuda:0')\n",
      "Min of x before attention: tensor(-30.4712, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(59.6321, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(90.2398, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(416.3615, device='cuda:0')\n",
      "Min of x before attention: tensor(-197.9111, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(439.4495, device='cuda:0')\n",
      "Min of x before attention: tensor(-194.5304, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(42.4037, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(119.3935, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1820.8486, device='cuda:0')\n",
      "Min of x before attention: tensor(-2432.4954, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1820.8645, device='cuda:0')\n",
      "Min of x before attention: tensor(-2433.4438, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5281, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2822, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(57.5086, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.4908, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.1883879154920578 max: 0.20893827080726624 std: 0.04419218748807907\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 3.40817862820586e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -255.27496337890625 531.9896850585938\n",
      "After stage2 NaN: False\n",
      "torch.Size([70, 96, 56, 56]) torch.Size([70, 192, 28, 28]) torch.Size([70, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(76.5169, device='cuda:0')\n",
      "Min of x before attention: tensor(-37.5044, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(82.0797, device='cuda:0')\n",
      "Min of x before attention: tensor(-38.6697, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(47.1414, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(149.8664, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(358.8842, device='cuda:0')\n",
      "Min of x before attention: tensor(-193.4334, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(378.9207, device='cuda:0')\n",
      "Min of x before attention: tensor(-193.4159, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(46.4567, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(136.9077, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1826.1241, device='cuda:0')\n",
      "Min of x before attention: tensor(-2442.4470, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1826.1400, device='cuda:0')\n",
      "Min of x before attention: tensor(-2443.4307, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5007, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2865, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(61.6452, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.5238, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.18723537027835846 max: 0.20768393576145172 std: 0.04419190064072609\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 3.0108919801818956e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -311.62982177734375 570.0621337890625\n",
      "After stage2 NaN: False\n",
      "torch.Size([70, 96, 56, 56]) torch.Size([70, 192, 28, 28]) torch.Size([70, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(92.1116, device='cuda:0')\n",
      "Min of x before attention: tensor(-34.1728, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(97.7530, device='cuda:0')\n",
      "Min of x before attention: tensor(-35.3381, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(38.7316, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(51.4415, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(285.0311, device='cuda:0')\n",
      "Min of x before attention: tensor(-311.6298, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(299.0939, device='cuda:0')\n",
      "Min of x before attention: tensor(-301.2452, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(44.2656, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(69.2921, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1852.8818, device='cuda:0')\n",
      "Min of x before attention: tensor(-2481.5273, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1852.8977, device='cuda:0')\n",
      "Min of x before attention: tensor(-2482.6482, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.4978, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2585, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(59.0866, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.5137, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.19158412516117096 max: 0.2121983915567398 std: 0.04419199377298355\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 2.500745388545056e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -333.8166809082031 610.6990966796875\n",
      "After stage2 NaN: False\n",
      "torch.Size([70, 96, 56, 56]) torch.Size([70, 192, 28, 28]) torch.Size([70, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(92.5389, device='cuda:0')\n",
      "Min of x before attention: tensor(-36.3867, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(93.2497, device='cuda:0')\n",
      "Min of x before attention: tensor(-37.5521, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(36.5981, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(60.4278, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(415.8628, device='cuda:0')\n",
      "Min of x before attention: tensor(-333.8167, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(431.3635, device='cuda:0')\n",
      "Min of x before attention: tensor(-328.5517, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(62.8089, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(83.0588, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1870.5894, device='cuda:0')\n",
      "Min of x before attention: tensor(-2515.2700, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1870.6052, device='cuda:0')\n",
      "Min of x before attention: tensor(-2516.4031, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5004, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2705, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(55.3401, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.4642, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.18945570290088654 max: 0.20963574945926666 std: 0.04419275000691414\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 3.0360709502019745e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -291.255126953125 511.8941345214844\n",
      "After stage2 NaN: False\n",
      "torch.Size([48, 96, 56, 56]) torch.Size([48, 192, 28, 28]) torch.Size([48, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(92.5160, device='cuda:0')\n",
      "Min of x before attention: tensor(-37.4159, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(101.6678, device='cuda:0')\n",
      "Min of x before attention: tensor(-38.5813, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(32.0548, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(44.0567, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(255.9471, device='cuda:0')\n",
      "Min of x before attention: tensor(-206.7575, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(264.8881, device='cuda:0')\n",
      "Min of x before attention: tensor(-182.8071, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(42.8742, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(120.8441, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1794.4904, device='cuda:0')\n",
      "Min of x before attention: tensor(-2397.9944, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1794.5062, device='cuda:0')\n",
      "Min of x before attention: tensor(-2398.9722, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.4956, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2870, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(57.8035, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.4871, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.19687148928642273 max: 0.21217499673366547 std: 0.0441899448633194\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 2.4433972622972533e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -300.7547912597656 386.0583801269531\n",
      "After stage2 NaN: False\n",
      "torch.Size([70, 96, 56, 56]) torch.Size([70, 192, 28, 28]) torch.Size([70, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(83.6936, device='cuda:0')\n",
      "Min of x before attention: tensor(-34.8320, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(89.4512, device='cuda:0')\n",
      "Min of x before attention: tensor(-35.9974, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(47.9059, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(57.2042, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(264.7083, device='cuda:0')\n",
      "Min of x before attention: tensor(-235.0349, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(280.0707, device='cuda:0')\n",
      "Min of x before attention: tensor(-229.2061, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(54.1455, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(117.3361, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1842.7496, device='cuda:0')\n",
      "Min of x before attention: tensor(-2469.6880, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1842.7655, device='cuda:0')\n",
      "Min of x before attention: tensor(-2470.5339, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5299, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2821, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(52.1363, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.4024, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.19708243012428284 max: 0.21521180868148804 std: 0.04419166222214699\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 3.0372520054555707e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -335.4720764160156 631.2159423828125\n",
      "After stage2 NaN: False\n",
      "torch.Size([60, 96, 56, 56]) torch.Size([60, 192, 28, 28]) torch.Size([60, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(104.5992, device='cuda:0')\n",
      "Min of x before attention: tensor(-36.3504, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(112.2698, device='cuda:0')\n",
      "Min of x before attention: tensor(-37.5157, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(41.5271, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(59.3801, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(510.3824, device='cuda:0')\n",
      "Min of x before attention: tensor(-327.3490, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(515.6122, device='cuda:0')\n",
      "Min of x before attention: tensor(-317.5410, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(56.5876, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(119.4300, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1858.2919, device='cuda:0')\n",
      "Min of x before attention: tensor(-2497.9131, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1858.3077, device='cuda:0')\n",
      "Min of x before attention: tensor(-2498.8035, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5541, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2658, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(57.8122, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.4919, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.19834169745445251 max: 0.21513263881206512 std: 0.0441923551261425\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 2.469231485946466e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -322.4643859863281 534.3406982421875\n",
      "After stage2 NaN: False\n",
      "torch.Size([70, 96, 56, 56]) torch.Size([70, 192, 28, 28]) torch.Size([70, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(94.7988, device='cuda:0')\n",
      "Min of x before attention: tensor(-33.5188, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(100.0445, device='cuda:0')\n",
      "Min of x before attention: tensor(-34.6842, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(46.1083, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(68.3372, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(460.9326, device='cuda:0')\n",
      "Min of x before attention: tensor(-235.3820, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(471.6491, device='cuda:0')\n",
      "Min of x before attention: tensor(-225.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(35.9049, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(75.9669, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1826.4116, device='cuda:0')\n",
      "Min of x before attention: tensor(-2444.7080, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1826.4275, device='cuda:0')\n",
      "Min of x before attention: tensor(-2445.7449, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5214, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2929, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(83.7726, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.5679, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.2066948413848877 max: 0.21641194820404053 std: 0.0441916398704052\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 2.726140424513801e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -283.6023864746094 377.0902404785156\n",
      "After stage2 NaN: False\n",
      "torch.Size([15, 96, 56, 56]) torch.Size([15, 192, 28, 28]) torch.Size([15, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(97.4036, device='cuda:0')\n",
      "Min of x before attention: tensor(-31.3454, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(102.3774, device='cuda:0')\n",
      "Min of x before attention: tensor(-32.5108, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(29.2668, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(55.5213, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(188.5451, device='cuda:0')\n",
      "Min of x before attention: tensor(-244.7581, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(196.9959, device='cuda:0')\n",
      "Min of x before attention: tensor(-239.6828, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(39.6349, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(87.0559, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1797.1283, device='cuda:0')\n",
      "Min of x before attention: tensor(-2394.9407, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1797.1442, device='cuda:0')\n",
      "Min of x before attention: tensor(-2395.9541, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.3098, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1934, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(57.3284, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.4781, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.20385749638080597 max: 0.21470847725868225 std: 0.04419175162911415\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 2.8401478946449288e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -267.1068420410156 445.482421875\n",
      "After stage2 NaN: False\n",
      "torch.Size([70, 96, 56, 56]) torch.Size([70, 192, 28, 28]) torch.Size([70, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(86.7486, device='cuda:0')\n",
      "Min of x before attention: tensor(-34.4874, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(86.9974, device='cuda:0')\n",
      "Min of x before attention: tensor(-35.6528, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(26.1708, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(37.1930, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(382.0214, device='cuda:0')\n",
      "Min of x before attention: tensor(-241.2243, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(391.9767, device='cuda:0')\n",
      "Min of x before attention: tensor(-224.7328, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(50.4284, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(138.4026, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1808.3315, device='cuda:0')\n",
      "Min of x before attention: tensor(-2408.0764, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1808.3474, device='cuda:0')\n",
      "Min of x before attention: tensor(-2409.0964, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.4951, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2656, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(60.9225, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.5145, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.19673864543437958 max: 0.21122761070728302 std: 0.04419104382395744\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 2.9339402018990768e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -347.83123779296875 705.134033203125\n",
      "After stage2 NaN: False\n",
      "torch.Size([35, 96, 56, 56]) torch.Size([35, 192, 28, 28]) torch.Size([35, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(107.8813, device='cuda:0')\n",
      "Min of x before attention: tensor(-26.3428, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(110.2370, device='cuda:0')\n",
      "Min of x before attention: tensor(-27.5026, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(45.9896, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(103.1655, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(352.5670, device='cuda:0')\n",
      "Min of x before attention: tensor(-248.1085, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(371.0635, device='cuda:0')\n",
      "Min of x before attention: tensor(-248.0909, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(40.3770, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(61.6760, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1801.4569, device='cuda:0')\n",
      "Min of x before attention: tensor(-2405.3276, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1801.4728, device='cuda:0')\n",
      "Min of x before attention: tensor(-2406.2800, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5130, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2869, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(55.3710, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.4609, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.20511142909526825 max: 0.21719278395175934 std: 0.044191207736730576\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 2.3644133317901606e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -356.9672546386719 646.8754272460938\n",
      "After stage2 NaN: False\n",
      "torch.Size([70, 96, 56, 56]) torch.Size([70, 192, 28, 28]) torch.Size([70, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(96.2187, device='cuda:0')\n",
      "Min of x before attention: tensor(-36.0000, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(101.0757, device='cuda:0')\n",
      "Min of x before attention: tensor(-37.1653, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(52.7063, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(66.5493, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(493.8657, device='cuda:0')\n",
      "Min of x before attention: tensor(-267.3565, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(501.7676, device='cuda:0')\n",
      "Min of x before attention: tensor(-263.9691, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(41.6410, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(121.9183, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1797.7970, device='cuda:0')\n",
      "Min of x before attention: tensor(-2400.2024, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1797.8129, device='cuda:0')\n",
      "Min of x before attention: tensor(-2401.3870, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5543, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2802, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(57.2849, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.4854, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.20357629656791687 max: 0.2181294709444046 std: 0.04419155418872833\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 2.3570920770765724e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -333.3764343261719 691.77099609375\n",
      "After stage2 NaN: False\n",
      "torch.Size([54, 96, 56, 56]) torch.Size([54, 192, 28, 28]) torch.Size([54, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(104.4800, device='cuda:0')\n",
      "Min of x before attention: tensor(-37.3145, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(108.0348, device='cuda:0')\n",
      "Min of x before attention: tensor(-38.2552, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(42.7750, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(56.2175, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(345.8855, device='cuda:0')\n",
      "Min of x before attention: tensor(-260.7807, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(358.9679, device='cuda:0')\n",
      "Min of x before attention: tensor(-246.5548, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(48.2933, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(76.2495, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1843.2893, device='cuda:0')\n",
      "Min of x before attention: tensor(-2471.7263, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1843.3052, device='cuda:0')\n",
      "Min of x before attention: tensor(-2472.5667, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5388, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2772, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(45.8710, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.3528, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.19964289665222168 max: 0.21755945682525635 std: 0.044192250818014145\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 3.3152625089627463e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -293.69427490234375 556.8814086914062\n",
      "After stage2 NaN: False\n",
      "torch.Size([12, 96, 56, 56]) torch.Size([12, 192, 28, 28]) torch.Size([12, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(90.4355, device='cuda:0')\n",
      "Min of x before attention: tensor(-33.3324, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(101.8386, device='cuda:0')\n",
      "Min of x before attention: tensor(-34.4977, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(35.4316, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(49.7445, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(278.4407, device='cuda:0')\n",
      "Min of x before attention: tensor(-221.3852, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(292.7810, device='cuda:0')\n",
      "Min of x before attention: tensor(-218.4155, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(33.7414, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(93.3322, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1791.8173, device='cuda:0')\n",
      "Min of x before attention: tensor(-2394.8984, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1791.8331, device='cuda:0')\n",
      "Min of x before attention: tensor(-2395.8740, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.2341, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2202, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(57.7005, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.4770, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.21464429795742035 max: 0.21938778460025787 std: 0.04419150948524475\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 3.799383030411718e-09 max: 0.9999997615814209\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -284.8219909667969 486.7236633300781\n",
      "After stage2 NaN: False\n",
      "torch.Size([70, 96, 56, 56]) torch.Size([70, 192, 28, 28]) torch.Size([70, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(100.2882, device='cuda:0')\n",
      "Min of x before attention: tensor(-33.1374, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(110.0940, device='cuda:0')\n",
      "Min of x before attention: tensor(-34.3028, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(50.3181, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(72.0539, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(378.3692, device='cuda:0')\n",
      "Min of x before attention: tensor(-162.0011, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(380.5987, device='cuda:0')\n",
      "Min of x before attention: tensor(-156.6088, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(74.0783, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(125.4614, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1822.6405, device='cuda:0')\n",
      "Min of x before attention: tensor(-2436.9487, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1822.6564, device='cuda:0')\n",
      "Min of x before attention: tensor(-2438.0520, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5057, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2966, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(59.7061, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.5082, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.20893996953964233 max: 0.21561050415039062 std: 0.04419040307402611\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 2.4398962850114003e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -321.4029235839844 546.333984375\n",
      "After stage2 NaN: False\n",
      "torch.Size([60, 96, 56, 56]) torch.Size([60, 192, 28, 28]) torch.Size([60, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(85.0459, device='cuda:0')\n",
      "Min of x before attention: tensor(-32.0999, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(93.7728, device='cuda:0')\n",
      "Min of x before attention: tensor(-33.2652, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(25.0340, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(40.6793, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(378.3340, device='cuda:0')\n",
      "Min of x before attention: tensor(-235.0672, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(385.2823, device='cuda:0')\n",
      "Min of x before attention: tensor(-218.8574, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(40.6739, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(77.5842, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1826.2183, device='cuda:0')\n",
      "Min of x before attention: tensor(-2450.1243, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1826.2341, device='cuda:0')\n",
      "Min of x before attention: tensor(-2451.2104, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5318, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2815, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(57.1663, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.4872, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.19310228526592255 max: 0.21355848014354706 std: 0.0441911481320858\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 2.1854156262435254e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -316.872314453125 456.66522216796875\n",
      "After stage2 NaN: False\n",
      "torch.Size([70, 96, 56, 56]) torch.Size([70, 192, 28, 28]) torch.Size([70, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(86.4247, device='cuda:0')\n",
      "Min of x before attention: tensor(-34.3930, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(92.6787, device='cuda:0')\n",
      "Min of x before attention: tensor(-35.5583, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(49.0822, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(65.3596, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(456.6652, device='cuda:0')\n",
      "Min of x before attention: tensor(-175.8458, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(467.9412, device='cuda:0')\n",
      "Min of x before attention: tensor(-167.2801, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(38.5876, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(86.6921, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1803.4851, device='cuda:0')\n",
      "Min of x before attention: tensor(-2410.3633, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1803.5010, device='cuda:0')\n",
      "Min of x before attention: tensor(-2411.4138, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5353, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2912, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(58.2737, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.4886, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.20052096247673035 max: 0.21583038568496704 std: 0.044190745800733566\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 2.1803778782469863e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -352.5976867675781 681.0557861328125\n",
      "After stage2 NaN: False\n",
      "torch.Size([70, 96, 56, 56]) torch.Size([70, 192, 28, 28]) torch.Size([70, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(103.2498, device='cuda:0')\n",
      "Min of x before attention: tensor(-36.6697, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(110.5493, device='cuda:0')\n",
      "Min of x before attention: tensor(-37.8351, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(59.4545, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(59.0071, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(340.5279, device='cuda:0')\n",
      "Min of x before attention: tensor(-245.4175, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(354.9027, device='cuda:0')\n",
      "Min of x before attention: tensor(-245.0337, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(33.2410, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(59.1432, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1809.6957, device='cuda:0')\n",
      "Min of x before attention: tensor(-2420.1821, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1809.7115, device='cuda:0')\n",
      "Min of x before attention: tensor(-2421.2139, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5335, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.3129, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(50.8988, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.4149, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.2110479325056076 max: 0.219402015209198 std: 0.044191449880599976\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 2.463849346767688e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -403.2717590332031 639.9952392578125\n",
      "After stage2 NaN: False\n",
      "torch.Size([70, 96, 56, 56]) torch.Size([70, 192, 28, 28]) torch.Size([70, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(82.4067, device='cuda:0')\n",
      "Min of x before attention: tensor(-32.4861, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(82.5980, device='cuda:0')\n",
      "Min of x before attention: tensor(-33.6514, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(53.0660, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(106.4278, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(522.4953, device='cuda:0')\n",
      "Min of x before attention: tensor(-321.8847, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(534.5479, device='cuda:0')\n",
      "Min of x before attention: tensor(-314.8280, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(58.5668, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(125.6419, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1832.9950, device='cuda:0')\n",
      "Min of x before attention: tensor(-2450.4233, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1833.0109, device='cuda:0')\n",
      "Min of x before attention: tensor(-2451.4126, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5374, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2714, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(61.5651, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.5198, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.21097025275230408 max: 0.21792206168174744 std: 0.04419145733118057\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 2.476872706935751e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -287.62298583984375 372.2386474609375\n",
      "After stage2 NaN: False\n",
      "torch.Size([40, 96, 56, 56]) torch.Size([40, 192, 28, 28]) torch.Size([40, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(97.7592, device='cuda:0')\n",
      "Min of x before attention: tensor(-37.5978, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(101.5977, device='cuda:0')\n",
      "Min of x before attention: tensor(-38.7632, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(52.3048, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(67.2814, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(372.2386, device='cuda:0')\n",
      "Min of x before attention: tensor(-143.8115, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(379.5742, device='cuda:0')\n",
      "Min of x before attention: tensor(-142.9783, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(37.7439, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(55.8977, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1813.0110, device='cuda:0')\n",
      "Min of x before attention: tensor(-2438.7971, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1813.0269, device='cuda:0')\n",
      "Min of x before attention: tensor(-2439.5947, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.4039, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.3051, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(58.1249, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.4913, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.21699027717113495 max: 0.22066815197467804 std: 0.044187042862176895\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 2.643757657239121e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -382.94915771484375 703.0796508789062\n",
      "After stage2 NaN: False\n",
      "torch.Size([70, 96, 56, 56]) torch.Size([70, 192, 28, 28]) torch.Size([70, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(90.5959, device='cuda:0')\n",
      "Min of x before attention: tensor(-32.2123, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(90.9365, device='cuda:0')\n",
      "Min of x before attention: tensor(-33.3776, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(56.7803, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(101.5659, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(428.6234, device='cuda:0')\n",
      "Min of x before attention: tensor(-287.1324, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(460.6116, device='cuda:0')\n",
      "Min of x before attention: tensor(-276.6501, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(49.8268, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(97.6340, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1829.2861, device='cuda:0')\n",
      "Min of x before attention: tensor(-2446.9326, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1829.3020, device='cuda:0')\n",
      "Min of x before attention: tensor(-2447.7588, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5223, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2832, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(57.1613, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.4699, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.21064496040344238 max: 0.21944662928581238 std: 0.04419160261750221\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 2.245749142204545e-09 max: 0.9999998807907104\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -346.23382568359375 649.993896484375\n",
      "After stage2 NaN: False\n",
      "torch.Size([24, 96, 56, 56]) torch.Size([24, 192, 28, 28]) torch.Size([24, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(74.7840, device='cuda:0')\n",
      "Min of x before attention: tensor(-44.5621, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(74.9983, device='cuda:0')\n",
      "Min of x before attention: tensor(-45.7274, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(26.6122, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(31.6164, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(324.9969, device='cuda:0')\n",
      "Min of x before attention: tensor(-237.0505, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(339.7481, device='cuda:0')\n",
      "Min of x before attention: tensor(-209.3659, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(39.5928, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(79.1412, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1824.4105, device='cuda:0')\n",
      "Min of x before attention: tensor(-2441.8386, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1824.4264, device='cuda:0')\n",
      "Min of x before attention: tensor(-2442.8853, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.5270, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2759, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(31.5030, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.3224, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.1975918561220169 max: 0.21764393150806427 std: 0.0441930778324604\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 3.637333545469801e-09 max: 0.9999997615814209\n",
      "Reduction:  8\n",
      "Before stage2 NaN: False\n",
      "Before stage2 min/max: -315.7319030761719 576.1249389648438\n",
      "After stage2 NaN: False\n",
      "torch.Size([24, 96, 56, 56]) torch.Size([24, 192, 28, 28]) torch.Size([24, 384, 14, 14])\n",
      "Shallow NaN: False\n",
      "Mid NaN: False\n",
      "Deep NaN: False\n",
      "Max of x before attention: tensor(92.4161, device='cuda:0')\n",
      "Min of x before attention: tensor(-41.3433, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(99.3400, device='cuda:0')\n",
      "Min of x before attention: tensor(-42.5086, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(90.6078, device='cuda:0')\n",
      "Min of x before attention: tensor(-0.8843, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(111.7399, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.0509, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(408.0784, device='cuda:0')\n",
      "Min of x before attention: tensor(-258.4948, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(414.6667, device='cuda:0')\n",
      "Min of x before attention: tensor(-252.3814, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(47.8685, device='cuda:0')\n",
      "Min of x before attention: tensor(-1.1769, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(104.1680, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.1070, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1809.7031, device='cuda:0')\n",
      "Min of x before attention: tensor(-2414.1216, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(1809.7190, device='cuda:0')\n",
      "Min of x before attention: tensor(-2415.1892, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(6.4949, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.2904, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Max of x before attention: tensor(45.8140, device='cuda:0')\n",
      "Min of x before attention: tensor(-2.3446, device='cuda:0')\n",
      "x has Inf: False\n",
      "channel_attention NaN: False\n",
      "filter_attention NaN: False\n",
      "kernel_attention NaN: False\n",
      "Pool1 NaN: False\n",
      "Pool2 NaN: False\n",
      "Pool3 NaN: False\n",
      "Before Interpolation NaN: False\n",
      "After Interpolation NaN: False\n",
      "After Decoder NaN: False\n",
      "After Projection NaN: False\n",
      "Image Features stats: min: -0.1996033489704132 max: 0.21818940341472626 std: 0.04419276490807533\n",
      "Text Features stats: min: -2.8657093048095703 max: 9.694480895996094 std: 0.48561668395996094\n",
      "Softmax probs stats: min: 3.1184124171801386e-09 max: 0.9999998807907104\n"
     ]
    }
   ],
   "source": [
    "num_images = 30\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "window_size = 224\n",
    "stride = 224\n",
    "alpha = 0.8\n",
    "# Create a grid of subplots: 2 rows and 10 columns (or adjust as needed)\n",
    "fig, axes = plt.subplots(num_images, 2, figsize=(15, num_images * 2), dpi=200, tight_layout=True, frameon=False)\n",
    "img_id = None\n",
    "# Loop through the dataset and display images\n",
    "for i in range(num_images):\n",
    "    # Fetch next image from the iterator or dataset\n",
    "    if img_id is not None:\n",
    "        image, points, density, image_path = dataset[img_id]\n",
    "    else:\n",
    "        image, points, density, image_path = next(data_iter)\n",
    "\n",
    "    image_height, image_width = image.shape[-2:]\n",
    "    image = image.to(DEVICE)\n",
    "    image_name = os.path.basename(image_path[0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if stride is not None:  # Sliding window prediction.\n",
    "            pred_density = sliding_window_predict(model, image, window_size, stride)\n",
    "        else:\n",
    "            _, pred_density = model(image)\n",
    "        pred_count = pred_density.sum().item()\n",
    "        resized_pred_density = resize_density_map(pred_density, (image_height, image_width)).cpu()\n",
    "\n",
    "    image = normalize(image, mean=(0., 0., 0.), std=(1. / std[0], 1. / std[1], 1. / std[2]))\n",
    "    image = normalize(image, mean=(-mean[0], -mean[1], -mean[2]), std=(1., 1., 1.))\n",
    "    image = to_pil_image(image.squeeze(0))\n",
    "\n",
    "    density = density.squeeze().numpy()\n",
    "    resized_pred_density = resized_pred_density.squeeze().numpy()\n",
    "    points = points[0].numpy()\n",
    "\n",
    "    # Plot the Ground Truth Image (Left Column)\n",
    "    axes[i, 0].imshow(image)\n",
    "    axes[i, 0].axis(\"off\")\n",
    "    axes[i, 0].set_title(f\"{image_name}\\nGT count: {len(points)}\")\n",
    "    if len(points) > 0:\n",
    "        axes[i, 0].scatter(points[:, 0], points[:, 1], s=1, c=\"white\")\n",
    "    axes[i, 0].imshow(density, cmap=\"jet\", alpha=0.5)\n",
    "\n",
    "    # Plot the Prediction Image (Right Column)\n",
    "    axes[i, 1].imshow(image)\n",
    "    axes[i, 1].imshow(resized_pred_density, cmap=\"jet\", alpha=0.5)\n",
    "    axes[i, 1].axis(\"off\")\n",
    "    axes[i, 1].set_title(f\"Pred count: {pred_count:.2f}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 30\n",
    "\n",
    "# Create a grid of subplots: 2 rows and 10 columns (or adjust as needed)\n",
    "fig, axes = plt.subplots(num_images, 2, figsize=(15, num_images * 2), dpi=200, tight_layout=True, frameon=False)\n",
    "img_id = None\n",
    "# Loop through the dataset and display images\n",
    "for i in range(num_images):\n",
    "    # Fetch next image from the iterator or dataset\n",
    "    if img_id is not None:\n",
    "        image, points, density, image_path = dataset[img_id]\n",
    "    else:\n",
    "        image, points, density, image_path = next(data_iter)\n",
    "\n",
    "    image_height, image_width = image.shape[-2:]\n",
    "    image = image.to(DEVICE)\n",
    "    image_name = os.path.basename(image_path[0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if stride is not None:  # Sliding window prediction.\n",
    "            pred_density = sliding_window_predict(model, image, window_size, stride)\n",
    "        else:\n",
    "            pred_density = model(image)\n",
    "        pred_count = pred_density.sum().item()\n",
    "        resized_pred_density = resize_density_map(pred_density, (image_height, image_width)).cpu()\n",
    "\n",
    "    image = normalize(image, mean=(0., 0., 0.), std=(1. / std[0], 1. / std[1], 1. / std[2]))\n",
    "    image = normalize(image, mean=(-mean[0], -mean[1], -mean[2]), std=(1., 1., 1.))\n",
    "    image = to_pil_image(image.squeeze(0))\n",
    "\n",
    "    density = density.squeeze().numpy()\n",
    "    resized_pred_density = resized_pred_density.squeeze().numpy()\n",
    "    points = points[0].numpy()\n",
    "\n",
    "    # Plot the Ground Truth Image (Left Column)\n",
    "    axes[i, 0].imshow(image)\n",
    "    axes[i, 0].axis(\"off\")\n",
    "    axes[i, 0].set_title(f\"{image_name}\\nGT count: {len(points)}\")\n",
    "    if len(points) > 0:\n",
    "        axes[i, 0].scatter(points[:, 0], points[:, 1], s=1, c=\"white\")\n",
    "    axes[i, 0].imshow(density, cmap=\"jet\", alpha=0.5)\n",
    "\n",
    "    # Plot the Prediction Image (Right Column)\n",
    "    axes[i, 1].imshow(image)\n",
    "    axes[i, 1].imshow(resized_pred_density, cmap=\"jet\", alpha=0.5)\n",
    "    axes[i, 1].axis(\"off\")\n",
    "    axes[i, 1].set_title(f\"Pred count: {pred_count:.2f}\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLRClip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
